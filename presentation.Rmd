---
title: "Local Polynomial Regression"
subtitle: "Statistical Machine Learning - Individual project"
author: "Leonardo Stincone"
date: "18th July 2019"
output:
  beamer_presentation:
    theme: "Pittsburgh"
    colortheme: "seahorse"
    fonttheme: "professionalfonts"
    includes:
      in_header: mypreamble.tex
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r libraries, include = F}
# Libraries ####
library(xtable)
library(mgcv)
library(splines)
library(MASS)
library(tidyverse)
library(gridExtra)
library(sm)
library(SemiPar)   # For lidar dataset

# Set plot themes
theme_set(theme_bw())

set.seed(42)
```


## Problem statement: Lidar dataset

```{r readData}
# cmb <- read.table("data/wmap.dat",header=TRUE)
# lidar <- read.table("data/lidar.dat",header=TRUE)
# lidar <- lidar[sort.list(lidar$range),]
# bpd <- read.table("data/bpd.dat",header=TRUE)

data(lidar)
lidar <- lidar[sort.list(lidar$range),]
```


\begincols
\begincol{.42\textwidth}
```{r lidarPlot, fig.height=5, fig.width=5}
ggplot(data = lidar,
       aes(x = range, y = logratio)) +
  geom_point() +
  labs(title = "Lidar dataset")
```
\endcol
\begincol{.58\textwidth}

LIDAR = LIght Detection And Ranging

* it is a surveying method that measures distance to a target by illuminating the target with laser light and measuring the reflected light with a sensor
* $x$: distance travelled before the light is reflected back to its source
* $y$: logarithm of the ratio of received light from two laser sources

\endcol
\endcols

The objective is to estimate
$$
f(x) = E \left[ Y \mid X=x \right]
$$



## What does local means?

\begincols
\begincol{.52\textwidth}

\small If we had enought point with $x=x_0$

```{r local, fig.height = 5, fig.width = 5}
par(mar = c(2,1,1,1))

n = 20
h = .1

# x = sort(runif(n, 0, 1))

x0 = 0.5
x = rep(x0, n)
ff = function(x){sqrt(3*x + .5)}
m = ff(x)
y = m + rnorm(n,0,0.1)

xlim = c(0,1)
# ylim = c(min(y) - .05 * (max(y) - min(y)),
         # max(y) + .05 * (max(y) - min(y)))
ylim = c(.5, 2.1)

plot(x, y, xlim = xlim, ylim = ylim, yaxt = "n", xaxt = "n",
     xlab = "", ylab = "", yaxs = "i",
     col = "blue", pch = 20)

# axis(1, at = c(x0-h, x0+h), labels = c("", ""))
axis(1, at = x0, labels = expression(x[0]))

# rect(x0 - h, ylim[1], x0 + h, ylim[2],
#      lwd=2, col = hsv(184/360, .57, .98, alpha = 0.2), border=NA)


curve(ff(x), add = TRUE, lwd = 1,
      xlim = c(-.1, 1.1))

# ind <- abs(x-x0) < h
# points(x[ind], y[ind], col = "blue", pch = 20)

segments(x0, -1, x0, mean(y), lwd = .5, lty = 2)
points(x0, mean(y), col = "red", pch = 20, cex = 2.5)

# lines(c(x0-h, x0+h), ylim[c(1,1)],
#       lwd = 2, col = "blue")
```
\endcol
\begincol{.52\textwidth}

<!-- \uncover<2->{ -->
\small We can consider points "close" to $x_0$

```{r neighborhood, fig.height = 5, fig.width = 5}

par(mar = c(2,1,1,1))

n = 100
h = .1

x = sort(runif(n, 0, 1))
x0 = 0.5
ff = function(x){sqrt(3*x + .5)}
m = ff(x)
y = m + rnorm(n,0,0.01)

xlim = c(0,1)
# ylim = c(min(y) - .05 * (max(y) - min(y)),
#          max(y) + .05 * (max(y) - min(y)))
ylim = c(.5, 2.1)

plot(x, y, xlim = xlim, ylim = ylim, yaxt = "n", xaxt = "n",
     xlab = "", ylab = "", yaxs = "i")

axis(1, at = c(x0-h, x0+h), labels = c("", ""))
axis(1, at = x0, labels = expression(x[0]))

rect(x0 - h, ylim[1], x0 + h, ylim[2],
     lwd=2, col = hsv(184/360, .57, .98, alpha = 0.2), border=NA)


curve(ff(x), add = TRUE, lwd = 1,
      xlim = c(-.1, 1.1))

ind <- abs(x-x0) < h
points(x[ind], y[ind], col = "blue", pch = 20)

segments(x0, -1, x0, mean(y[ind]), lwd = .5, lty = 2)
points(x0, mean(y[ind]), col = "red", pch = 20, cex = 2.5)

lines(c(x0-h, x0+h), ylim[c(1,1)],
      lwd = 2, col = "blue")
```
<!-- } -->
\endcol
\endcols






## 2 basic ideas


## Nadaraya-Watson kernel regression


## Some kernels proposed



## Nadaraya-Watson estimator issues



## Local polynomial regression


































